{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering & Bias/Variance Tradeoff\n",
    "\n",
    "<img src=https://i.redd.it/ngdmak09ha131.jpg width=400>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson objectives:\n",
    "\n",
    "By the end of this lesson students will be able to:\n",
    "- Understand how to create new variables based using other variables in our dataset\n",
    "- Understand and apply feature scaling techniques\n",
    "- Understand what Bias and Variance are in terms of data modeling \n",
    "- Understand what model underfitting and overfitting are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets take a look at the Multiple Linear Regression below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.formula.api import ols\n",
    "import seaborn as sns\n",
    "from functions import * \n",
    "%matplotlib inline\n",
    "\n",
    "baseball = pd.read_csv('data/baseball_height_weight.csv')\n",
    "baseball.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets run our OLS model for weight_ib on height_in and age\n",
    "\n",
    "lr = ols(formula='weight_lb~age+height_in', data=baseball).fit()\n",
    "lr.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turn and Talk!\n",
    "\n",
    "With your group:\n",
    "\n",
    "1. Interpret the coefficient values from our model\n",
    "2. Given this interpretation which feature is more important to predicting the target variable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Feature Scaling?\n",
    "Scaling data is the process of **increasing or decreasing the magnitude according to a fixed ratio.** You change the size but not the shape of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(baseball['height_in'].values.reshape(-1, 1))\n",
    "\n",
    "ax1 = plt.subplot(1, 2, 1)\n",
    "baseball['height_in'].hist(ax = ax1)\n",
    "ax1.set_title('Original Data')\n",
    "\n",
    "ax2 = plt.subplot(1, 2, 2)\n",
    "ax2.hist(scaled_data)\n",
    "ax2.set_title('Scaled Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^^ Notice the change in the X-axis' scale. But NOT the shape of the distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why do we need to use feature scaling?\n",
    "\n",
    "- In order to compare the magnitude of coefficients thus increasing the interpretability of coefficients.\n",
    "- It helps handling disparities in units.\n",
    "- Some models use euclidean distance in their computations.\n",
    "- Some models require features to be on equivalent scales.\n",
    "- In the machine learning space, it helps improve the performance of the model and reducing the values/models from varying widely.\n",
    "- Some algorithms are sensitive to the scale of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When do we use feature scaling?\n",
    "- in the preprocessing phase.\n",
    "\n",
    "## How do we perform feature scaling?\n",
    "\n",
    "The common types of feature scaling are:\n",
    "\n",
    "- Min/Max Scaling\n",
    "- Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [MinMax Scalar](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html?highlight=minmax#sklearn.preprocessing.MinMaxScaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "X_{norm} & = \\frac{X - X_{min}}{X_{max}-X_{min}} \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import MinMaxScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Instantiate MinMaxScaler object\n",
    "minmax = MinMaxScaler()\n",
    "\n",
    "\n",
    "# Set X to standardized features\n",
    "X=baseball[['height_in', 'age']]\n",
    "\n",
    "#scale our X variables\n",
    "X_scaled = minmax.fit_transform(X)\n",
    "\n",
    "baseball['scaled_height'] = X_scaled[:, 0]\n",
    "baseball['scaled_age'] = X_scaled[:, 1]\n",
    "\n",
    "\n",
    "# Using Ordinary Least Squares (OLS - from StatsModel), fit a model to our X and y.\n",
    "lr_scaled = ols(formula='weight_lb~scaled_age+scaled_height', data=baseball).fit()\n",
    "\n",
    "\n",
    "# Output the summary of the model. Label the coefficients as below. \n",
    "lr_scaled.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets interpret this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1 unit increase in height estimates +79.56 lbs of weight\n",
    "- 1 unit increase in age estimates +26.6 lbs of weight\n",
    "\n",
    "Makes it easier to compare different quantity measurements with each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now try applying Standardization to the same dataset\n",
    "\n",
    "The most common method of scaling is standardization.  In this method we center the data, then we divide by the standard devation to enforce that the standard deviation of the variable is one:\n",
    "\n",
    "$X_{std} = \\cfrac{X-\\bar{X}}{s_X}$\n",
    "\n",
    "\n",
    "With standardization we now interpret the coefficients in units of standard deviation of each variable!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn\n",
    "\n",
    "With your group:\n",
    "\n",
    "1. Apply standardization to age and height variables using [`StandardScaler` in sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html?highlight=standard%20scaler#sklearn.preprocessing.StandardScaler).\n",
    "2. Re-interpret the coefficient values\n",
    "3. Given your interpretation of the coefficients, which feature is more important at predicting the target?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your Interpretation here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How do we implement scaling with sklearn and a test-train split?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import appropriate models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "# Read in CSV and set X and y. \n",
    "baseball = pd.read_csv('data/baseball_height_weight.csv')\n",
    "X = baseball[['height_in','age']]\n",
    "y = baseball['weight_lb']\n",
    "\n",
    "# Run the train-test split function.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,               # Pass in our X and y\n",
    "                                                    random_state=42,    # Abritary select a random_state \n",
    "                                                    test_size=.2        # Split test size to be 20% of full data.\n",
    "                                                   )\n",
    "# Instantiate the Standar Scaler Object.\n",
    "ss = StandardScaler()\n",
    "\n",
    "# Fit and Transform the training data.\n",
    "X_train_transformed = ss.fit_transform(X_train)\n",
    "\n",
    "# ONLY TRANSFORM the test data.\n",
    "X_test_transformed = ss.transform(X_test)\n",
    "\n",
    "# Instantiate the Linear Regression Object.\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model to the transformed X_train, and the y_train.\n",
    "model.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Score the model based on the transformed X_test and the y_test.\n",
    "model.score(X_test_transformed, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check out these visualizations to better wrap your head around the affects of scaling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import * \n",
    "make_plot(0) # Unscaled Data\n",
    "make_plot(1) # Standard Scaled\n",
    "make_plot(2) # MinMax Scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn!\n",
    "\n",
    "In your group:\n",
    "\n",
    "1. Create a linear regression model to predict gross revenue.  Use the variables `cast_total_facebook_likes` and `budget` as your features.\n",
    "2.  Using a test-train split, run your model on your raw training data.\n",
    "3.  Run your model again using standard scaling of your training features.\n",
    "4.  Run your model again using min-max scaling of your training features.\n",
    "5.  What about the model summary changed between these three models?  Was r-squared impacted?  Coefficients?  P-values for features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating new variables\n",
    "\n",
    "\n",
    "There are some times in which we might want to create a new feature in our dataset.  For example we might want to create a ratio from our columns.  There are also times in which we might want to bin, or categorize, a continuous variable.\n",
    "\n",
    "Let's do each of these!\n",
    "\n",
    "First let's create a ratio of years played/age of each player to see the proportion of their life they have played baseball."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseball['proportion_age_played'] = baseball.years_played/baseball.age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseball.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might also want to test a hypothesis that older players weight more than younger players.  In order to do this we would need to bin our age variable.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseball.age.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turn and talk\n",
    "\n",
    "1. How should we bin this variable?  What would be consider to be young vs old?\n",
    "2. Create your bins!\n",
    "3. Run another regression model using your binned age variable and your new variable about the proportion of time played as features.  Remember to take out your original age predictor!\n",
    "4.  Interpret the coefficients of your features.  Examine the r-squared value.  Is this model a better fit than our previous model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias Variance Tradeoff\n",
    "\n",
    "<img src='https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcQpAMlAGFzVmJ0es77DsYx9qE0dhObJbsNL3A&usqp=CAU' width =300>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![which model is better](img/which_model_is_better.png)\n",
    "\n",
    "https://towardsdatascience.com/cultural-overfitting-and-underfitting-or-why-the-netflix-culture-wont-work-in-your-company-af2a62e41288\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What makes a model good?\n",
    "\n",
    "- We don’t ultimately care about how well your model fits your data.\n",
    "\n",
    "- What we really care about is how well your model describes the process that generated your data.\n",
    "\n",
    "- Why? Because the data set you have is but one sample from a universe of possible data sets, and you want a model that would work for any data set from that universe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Error: prediction error and irreducible error\n",
    "\n",
    "\n",
    "\n",
    "### Regression fit statistics are often called “error”\n",
    " - Sum of Squared Errors (SSE)\n",
    " $ {\\displaystyle \\operatorname {SSE} =\\sum _{i=1}^{n}(Y_{i}-{\\hat {Y_{i}}})^{2}.} $\n",
    " - Mean Squared Error (MSE) \n",
    " \n",
    " $ {\\displaystyle \\operatorname {MSE} ={\\frac {1}{n}}\\sum _{i=1}^{n}(Y_{i}-{\\hat {Y_{i}}})^{2}.} $\n",
    " \n",
    " - Root Mean Squared Error (RMSE)  \n",
    " $ {\\displaystyle \\operatorname \n",
    "  {RMSE} =\\sqrt{MSE}} $\n",
    "\n",
    " All are calculated using residuals    \n",
    "\n",
    "![residuals](img/residuals.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This error can be broken up into parts:\n",
    "\n",
    "![defining error](img/defining_error.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There will always be some random, irreducible error inherent in the data.  Real data always has noise.\n",
    "\n",
    "The goal of modeling is to reduce the prediction error, which is the difference between our model and the realworld processes from which our data is generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction error is a combination of bias and variance\n",
    "\n",
    "\n",
    "$ Total\\ Error\\ = Prediction\\ Error+ Irreducible\\ Error$\n",
    "\n",
    "Our prediction error can be further broken down into error due to bias and error due to variance.\n",
    "\n",
    "$ Total\\ Error = Model\\ Bias^2 + Model\\ Variance + Irreducible\\ Error$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Bias** is the expected prediction error of the expected trained model\n",
    "\n",
    "> In other words, if you were to train multiple models on different samples, what would be the average difference between the prediction and the real value.\n",
    "\n",
    "**Model Variance** is the expected variation in predictions, relative to your expected trained model\n",
    "\n",
    "> In other words, what would be the average difference between any one model's prediction and the average of all the predictions .\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thought Experiment\n",
    "\n",
    "1. Imagine you've collected 23 different training sets for the same problem.\n",
    "2. Now imagine training one model on each of your 23 training sets.\n",
    "3. Bias vs. variance refers to the accuracy vs. consistency of the models trained by your algorithm.\n",
    "\n",
    "![target_bias_variance](img/target.png)\n",
    "\n",
    "http://scott.fortmann-roe.com/docs/BiasVariance.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Bias Variance Tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**High bias** algorithms tend to be less complex, with simple or rigid underlying structure.\n",
    "\n",
    "+ They train models that are consistent, but inaccurate on average.\n",
    "+ These include linear or parametric algorithms such as regression and naive Bayes.\n",
    "+ For linear, perhaps some assumptions about our feature set could lead to high bias. \n",
    "      - We did not include the correct predictors\n",
    "      - We did not take interactions into account\n",
    "      - In linear, we missed a non-linear relationship (polynomial). \n",
    "      \n",
    "High bias models are **underfit**\n",
    "\n",
    "On the other hand, **high variance** algorithms tend to be more complex, with flexible underlying structure.\n",
    "\n",
    "+ They train models that are accurate on average, but inconsistent.\n",
    "+ These include non-linear or non-parametric algorithms such as decision trees and nearest neighbors.\n",
    "+ For linear, perhaps we included an unreasonably large amount of predictors using polynomial regression. \n",
    "+ High variance models are modeling the noise in our data\n",
    "\n",
    "High variance models are **overfit**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we build our models, we have to keep this relationship in mind.  If we build complex models, we risk overfitting our models.  Their predictions will vary greatly when introduced to new data.  If our models are too simple, the predictions as a whole will be inaccurate.   \n",
    "\n",
    "The goal is to build a model with enough complexity to be accurate, but not too much complexity to be erratic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![optimal](img/optimal_bias_variance.png)\n",
    "http://scott.fortmann-roe.com/docs/BiasVariance.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![which_model](img/which_model_is_better_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use of Test-Train Split and Cross Validation\n",
    "\n",
    "It is hard to know if your model s too simple or too complex by just using it on the training data. This is why we withhold some of our data as a test set!\n",
    "\n",
    "The using our model with our test data allows us to evaluate if our model has the right balance of variance vs bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How do we know if our model is overfitting or underfitting?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our model is not performing well on the training  data, we are probably underfitting it.  \n",
    "\n",
    "\n",
    "To know if our  model is overfitting the data, we need  to test our model on unseen data. \n",
    "We then measure our performance on the unseen data. \n",
    "\n",
    "If the model performs way worse on the  unseen data, it is probably  overfitting the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://developers.google.com/machine-learning/crash-course/images/WorkflowWithTestSet.svg' width=500/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Should you ever fit on your test set?  \n",
    "\n",
    "\n",
    "![no](https://media.giphy.com/media/d10dMmzqCYqQ0/giphy.gif)\n",
    "\n",
    "\n",
    "**Never fit on test data.** If you are seeing surprisingly good results on your evaluation metrics, it might be a sign that you are accidentally training on the test set. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
